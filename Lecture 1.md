# Deep Reinforcement Learning

## An Introduction

深度强化学习是一个很大的题目。有的人认为，AI就是深度学习（Deep Learning）加上强化学习（Reinforcement Learning）。这两者的结合也可以被称为深度强化学习。强化学习由两部分构成：智能体和环境。在强化学习的过程中，智能体和环境会一直交互。

在深度强化学习里面，智能体（agent）会去观察这个环境（environment），得到自己的状态（state）。但状态或许用观察结果（observation）替代更合适，因为他所指的并不是整个系统所处于的状态，而仅仅是智能体所观察到的部分。

> 所以会有 Partially observable state 这个概念，因为机器有可能不能观察到所有的状态。

在了解到状态后，智能体会去做出行动（action），这个行动会改变所处的环境。进而，智能体会获得结果（reword）。根据结果是正面（positive）或负面（negative），机器得以进行学习并改变自己在所处环境下的策略。

#### Supervised Learning

上述的学习方式与监督式学习（Supervised Learning）似乎有着相似性，都是根据结果（监督式学习中的label）调整自己的行为策略。但这两者还是会存在一定程度的不同。

对于监督学习而言，输入的数据应当是没有关联的。而强化学习中，由于智能体会连续不断的改变环境，上一次输入的数据会与下一次有很强的关联性。此外，监督学习需要告诉智能体，正确的标签是什么，这样才可以让智能体修正自己的预测，进行正确的学习。而在强化学习中，机器不会得到立刻的反馈。

> 强化学习之所以困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体可以借由这个环境进行学习。

以下围棋为例：监督式学习的方式是向智能体输入一个盘式，并告诉智能体，在这一环境下，应当在什么地方落子。而对于强化学习而言，我们需要考虑到，给定的标签（棋谱）可能不是最优的。人也不一定知道在特定环境下的最优解法。所以会让智能体寻找一个对弈的人（有可能也是智能体），进行许多盘游戏。赢了就得到正向反馈，输了得到负向。

强化学习与监督学习的区别可以被归结为一下几点：

- 强化学习处理的大多数是序列数据，其很难像监督学习的样本一样满足独立同分布。
- 强化学习中并没有明确的标签，不会有数据指示每一步的正确动作是什么，只会在游戏结束时获得反馈。智能体需要自己去发现哪些动作（或组合）可以带来最大的奖励。
- 智能体学习的过程，其实是不断的试错和探索的过程。探索（exploration）和利用（exploitation）是强化学习中非常重要的问题。

#### Difficulties

在强化学习的训练过程中，时间非常重要。因为我们得到的是与时间有关联的数据。

- Reword Delay

还是以下围棋为例：现在下的一步，可能会在很久以后得到优秀的结果，机器需要有足够的远见才会考虑采取这样的一步。

- Actions Affect

智能体所采取的行为会改变环境，这会导致问题复杂度的提升。另一方面，智能体必须要学会自己探索这个环境，不然会处于一种，因为不知道这样做的收获，所以不将其列入考虑的策略的困境。探索尚未进行的行为也是智能体重要的功能之一。

> 这是否可以被归结于初始时对智能体的设置不合理？或许策略的采取也是人类难以完善归结的部分之一。

#### History

作为本节标题的深度强化学习并不是强化学习最早的产品。最初的强化学习是通过定义特征描述状态，随后就可以训练一个分类网络或者价值估计函数来采取行动。但有了深度学习后，我们就可以用一个神经网络来拟合价值函数或策略网络，免去特征设计的部分。深度强化学习将训练过程变成了一个端到端的行为。

近些年，随着算力和基础设备的不断提高，智能体可以在环境中获得更多的信息。这使得强化学习发展迅速。

## Terminology

在更严谨的学习深度强化学习之前，我们有必要严格的定义一下所要用到的术语。

###### Agent & Environment

强化学习研究的是智能体（Agent）和环境（Environment）交互的问题。智能体观测环境，获取到观测结果，做出行为。环境受到行为的影响而发生变化。智能体继续观测下一步并依据事先制定好的规则获取奖励。

对于一个强化学习智能体而言，它可能有一个或多个如下的组成部分：

- 策略（Policy）。智能体采用策略选取下一步的动作。

策略是智能体的动作模型，它决定了智能体会采取的动作。策略可以分为随机性策略和确定性策略，前者输出一个概率，对概率分布进行采样即可得到智能体的行为。后者输出智能体将要采取的动作。通常情况下，强化学习会采取随机性策略。

- 价值函数（Value Function）。智能体用价值函数评估当前状态。

价值函数会评估当前状态，从而对未来的奖励做出预测。价值函数中存在一个折扣因子，用于描述奖励随时间的变化。描述奖励随状态和动作而变化的被称为Q函数。

- 模型（Model）。模型表示智能体对环境状态的理解。

模型决定了下一步的状态，它是智能体对环境的建模。下一步的状态取决于当前的状态以及当前采取的状态。它由状态转移概率和奖励函数两部分组成。状态转移概率用于描述外部状态发生变化的概率，比如在当前状态下，有多大的概率会转变到状态A，多大概率转变为状态B。奖励函数描述了我们在当前状态下，采取特定行径所能得到的奖励。

当我们有了策略、价值函数和模型后，就形成了一个马尔可夫决策过程。

###### Reward

奖励（Reward）是由环境给的一种标量的反馈信号，这种信号可显示智能体在某一步采取某个策略的表现如何。智能体的目的是最大化所获奖励之和。

###### Obsveration

观测结果（Observation）是智能体对于环境某一时刻状态（State）的描述。其与状态的重要区别在于：状态是对智能体所处环境的完整描述，而观测结果是对所处环境的部分描述（有些时候也可以是完整的）。

###### Action & Action Space

动作（Action）是在智能体观测到环境后，做出的行为。在给定的环境中，有效动作的合集被称为动作空间（Action Apace）。

###### Decision sequence

在一个强化学习环境中，智能体的目的是选取一系列的动作来最大化奖励，所以选取的动作必须有长期的影响。虽然智能体在每一步后都可以获得一个收益，但这个收益并不能完整的描述这一动作的效果。部分动作的效果会在更长的周期中表现出来。强化学习的一个重要课题就是近期与远期奖励之间的权衡（trade-off），研究怎么让智能体获得更多的总体收益。

在于环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体采取一个动作，也会获得一个奖励。所以智能体所经历的历史是观测、动作、奖励的序列。
$$
History = \left \{ s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},...,s_{t},a_{t},r_{t} \right \}
$$
智能体在决策当前动作的时候会依赖于此前得到的历史，而智能体此前的历史又会影响到当前环境的状态。所以，我们可以将整个游戏的状态看成关于这个历史的函数：
$$
s_t = f(History)
$$
环境会有自己的函数来更新状态，智能体内部也会有函数来更新其对环境的观测结果。当智能体能够观察到环境的所有状态时，我们认为这个环境时完全可观测的。在这种情况下，强化学习可以被建模为一个马尔可夫决策过程。

但是，同样存在智能体只能观测到环境的部分状态。这种情况下，我们称环境是部分可观测的。进而，我们将强化学习建模部分可观测马尔可夫决策过程。它是马尔可夫决策过程的一种泛化。

## Agent Types

### Policy-based & Value-based

![RL-Type](.\pic\RL-Type.png)

虽然这里将强化学习分为了两类，但其实用起来往往会两者结合。甚至采用更多的方式，如model-based。

#### Policy-based & Value-based

根据智能体学习的事物不同，我们可以将智能体进行归类。

基于价值的智能体（Value-based）显式的学习价值函数，隐式的学习策略。策略是其从学到的价值函数中推算出来的。通常而言，智能体维护一个价值表格或价值函数，并通过这个表格或者函数来选择价值最大的策略。这一般被应用于离散的环境下，对于动作集合规模庞大，动作连续的常见，很难获得好的效果。

基于策略的智能体（Policy-based）直接学习策略，它没有学习价值函数。智能体会制定一套动作策略，确定在给定环境下需要采取何种动作，并根据这个策略进行操作。学习的过程中，智能体针对策略进行优化，使得指定的策略能够获得最大的奖励。

#### model-based & model free

此外，我们还可以通过智能体是否学习环境模型来对智能体进行分类。有模型强化学习智能体（model-based）通过学习状态的转移来采取动作。无模型强化学习智能体（model-free）没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数来进行决策。

在实际应用中，如果不清楚改用有模型强化学习还是无模型强化学习，可以先思考在智能体执行动作前，是否能对下一步的状态和奖励进行预测。如果能，就能够对环境进行建模，从而采用有模型强化学习。

虽然无模型强化学习只比有模型强化学习少一步。实际上，无模型强化学习需要远比有模型强化学习多的多的数据，大量的采样来估计状态、动作和奖励函数，从而优化动作策略。这也使得无模型强化学习有更强的泛化能力。

#### Machine Learning ≈ Looking for a Function

我们可以将机器学习的训练过程理解为智能体在寻找一个方法（Function），使其可以在收到输入的时候得出期望输出。在强化学习中，我们可以将这一过程表达为下述公式：
$$
Action = Function(Observation)
$$
这里的方法，也被称为Actor或者Policy。如果我们用一个神经网络作为Actor，那么输入就是智能体观察到的状态（处理成神经网络可以接受的向量或者矩阵），输出就是神经网络输出的值。

在强化学习中，评价Actor的好坏自然就是最终的结果。在一轮游戏（episodes）中采用这一策略（在下文公式中用$\theta$表示） ，并衡量获取到的结果之和。

> 但是，考虑到游戏本身具备的随机性，以及如果采用的Actor是概率类的模型。那么单次游戏的结果也不应该作为最终的衡量标准，而应该选取得分的期望值。

我们可以将单次游戏得到的结果用$\tau$进行表述：
$$
\tau = \left \{ s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},...,s_{N},a_{N},r_{N} \right \}
$$
其中，定义$s$为状态，$a$为状态下的行动（action），$r$为行动后获得的结果。

而最终衡量的目标，如前文所述，是单步获得的Reward之和：
$$
R(\tau) =  {\textstyle \sum_{n=1}^{N}} (r_{n}) 
$$
所寻求的得分期望值，便可以被表示为：
$$
\bar{ R_{\theta} }  =  {\textstyle \sum_{\tau}^{} R(\tau)P(\tau|\theta)} \approx \frac{1}{N} {\textstyle \sum_{n=1}^{N} R(\tau_{n})}
$$
既然目标确定了（寻找合适的参数$\theta$，最大化目标值）。采用的方式便是梯度下降：随机初始化一个$\theta$，采用梯度下降以获取最大的目标值。我们希望达到的目的是（用比最大化目标期望值更直接的方式）：

*如果在单次游戏$\tau^{n}$中，智能体在状态$s_{t}^{n}$下采用的行为$a_{t}^{n}$使得最终结果$R(\tau^{n})$为正反馈，则调整策略$\theta$以最大化概率$p(a_{t}^{n}|s_{t}^{n})$。如果是负反馈，则减小概率。*

> 注意，这里要最大化的是最终总体收益，而非单步收益。

###### Policy Gradient







