# Deep Reinforcement Learning

## An Introduction

深度强化学习是一个很大的题目。有的人认为，AI就是深度学习（Deep Learning）加上强化学习（Reinforcement Learning）。这两者的结合也可以被称为深度强化学习。

在深度强化学习里面，智能体（agent）会去观察这个环境（environment），得到自己的状态（state）。但状态或许用观察结果（observation）替代更合适，因为他所指的并不是整个系统所处于的状态，而仅仅是智能体所观察到的部分。

> 所以会有 Partially observable state 这个概念，因为机器有可能不能观察到所有的状态。

在了解到状态后，智能体会去做出行动（action），这个行动会改变所处的环境。进而，智能体会获得结果（reword）。根据结果是正面（positive）或负面（negative），机器得以进行学习并改变自己在所处环境下的策略。

###### Supervised Learning

上述的学习方式与监督式学习（Supervised Learning）似乎有着相似性，都是根据结果（监督式学习中的label）调整自己的行为策略。但这两者还是会存在一定程度的不同。

以下围棋为例：监督式学习的方式是向智能体输入一个盘式，并告诉智能体，在这一环境下，应当在什么地方落子。而对于强化学习而言，我们需要考虑到，给定的标签（棋谱）可能不是最优的。人也不一定知道在特定环境下的最优解法。所以会让智能体寻找一个对弈的人（有可能也是智能体），进行许多盘游戏。赢了就得到正向反馈，输了得到负向。

> 这是否意味着，强化学习下的标签相比监督式学习更灵活？不会有“标签”告诉智能体，什么是好的，什么是不好的。智能体要自己学习。
>
> 虽然如此，强化学习下的agent依然有着基础的标签（作为反馈的最终结果）。除此之外，先使用监督式学习获得一定的基础，再进行强化学习，可以得到一定程度的便捷。

###### Difficulties

- Reword Delay

还是以下围棋为例：现在下的一步，可能会在很久以后得到优秀的结果，可机器需要有足够的远见才会考虑采取这样的一步。

- Actions Affect

智能体所采取的行为会改变环境，这会导致问题复杂度的提升。另一方面，智能体必须要学会自己探索这个环境，不然会处于一种，因为不知道这样做的收获，所以不将其列入考虑的策略的困境。探索尚未进行的行为也是智能体重要的功能之一。

> 这是否可以被归结于初始时对智能体的设置不合理？或许策略的采取也是人类难以完善归结的部分之一。

## Policy-based & Value-based

![RL-Type](.\pic\RL-Type.png)

虽然这里将强化学习分为了两类，但其实用起来往往会两者结合。甚至采用更多的方式，如model-based。

#### Policy-based

###### Machine Learning ≈ Looking for a Function

我们可以将机器学习的训练过程理解为智能体在寻找一个方法（Function），使其可以在收到输入的时候得出期望输出。在强化学习中，我们可以将这一过程表达为下述公式：
$$
Action = Function(Observation)
$$
这里的方法，也被称为Actor或者Policy。如果我们用一个神经网络作为Actor，那么输入就是智能体观察到的状态（处理成神经网络可以接受的向量或者矩阵），输出就是神经网络输出的值。

在强化学习中，评价Actor的好坏自然就是最终的结果。在一轮游戏（episodes）中采用这一策略（在下文公式中用$\theta$表示） ，并衡量获取到的结果之和。

> 但是，考虑到游戏本身具备的随机性，以及如果采用的Actor是概率类的模型。那么单次游戏的结果也不应该作为最终的衡量标准，而应该选取得分的期望值。

我们可以将单次游戏得到的结果用$\tau$进行表述：
$$
\tau = \left \{ s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},...,s_{N},a_{N},r_{N} \right \}
$$
其中，定义$s$为状态，$a$为状态下的行动（action），$r$为行动后获得的结果。

而最终衡量的目标，如前文所述，是单步获得的Reward之和：
$$
R(\tau) =  {\textstyle \sum_{n=1}^{N}} (r_{n}) 
$$
所寻求的得分期望值，便可以被表示为：
$$
\bar{ R_{\theta} }  =  {\textstyle \sum_{\tau}^{} R(\tau)P(\tau|\theta)} \approx \frac{1}{N} {\textstyle \sum_{n=1}^{N} R(\tau_{n})}
$$
既然目标确定了（寻找合适的参数$\theta$，最大化目标值）。采用的方式便是梯度下降：随机初始化一个$\theta$，采用梯度下降以获取最大的目标值。我们希望达到的目的是（用比最大化目标期望值更直接的方式）：

*如果在单次游戏$\tau^{n}$中，智能体在状态$s_{t}^{n}$下采用的行为$a_{t}^{n}$使得最终结果$R(\tau^{n})$为正反馈，则调整策略$\theta$以最大化概率$p(a_{t}^{n}|s_{t}^{n})$。如果是负反馈，则减小概率。*

> 注意，这里要最大化的是最终总体收益，而非单步收益。（贪心）

###### Policy Gradient







